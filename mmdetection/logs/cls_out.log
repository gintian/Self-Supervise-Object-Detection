nohup: ignoring input
/home/liyanni/anaconda3/envs/GPU-info/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps,
/home/liyanni/anaconda3/envs/GPU-info/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/home/liyanni/anaconda3/envs/GPU-info/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,
/home/liyanni/anaconda3/envs/GPU-info/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, positive=False):
/home/liyanni/anaconda3/envs/GPU-info/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/home/liyanni/anaconda3/envs/GPU-info/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,
/home/liyanni/anaconda3/envs/GPU-info/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, copy_X=True, positive=False):
/home/liyanni/anaconda3/envs/GPU-info/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  precompute=False, eps=np.finfo(np.float).eps,
/home/liyanni/anaconda3/envs/GPU-info/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=np.finfo(np.float).eps, random_state=None,
/home/liyanni/anaconda3/envs/GPU-info/lib/python3.7/site-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  eps=4 * np.finfo(np.float).eps, n_jobs=None,
/home/liyanni/anaconda3/envs/GPU-info/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:31: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  EPS = np.finfo(np.float).eps
2022-02-13 15:59:12,245 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.9 (default, Aug 31 2020, 12:42:55) [GCC 7.3.0]
CUDA available: True
GPU 0,1,2: Tesla P40
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.0, V10.0.130
GCC: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)
PyTorch: 1.7.1+cu92
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.6.0 (Git Hash 5ef631a030a6f73131c77892041042805a06064f)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 9.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70
  - CuDNN 7.6.3
  - Magma 2.5.2
  - Build settings: BLAS=MKL, BUILD_TYPE=Release, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, USE_CUDA=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.8.2+cu92
OpenCV: 4.5.2
MMCV: 1.4.3
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 9.2
MMDetection: 2.20.0+ff9bc39
------------------------------------------------------------

2022-02-13 15:59:12,613 - mmdet - INFO - Distributed training: False
2022-02-13 15:59:12,971 - mmdet - INFO - Config:
optimizer = dict(
    type='SGD',
    lr=0.00125,
    momentum=0.9,
    weight_decay=0.0005,
    nesterov=True,
    paramwise_cfg=dict(norm_decay_mult=0.0, bias_decay_mult=0.0))
optimizer_config = dict(grad_clip=None)
lr_config = dict(
    policy='YOLOX',
    warmup='exp',
    by_epoch=False,
    warmup_by_epoch=True,
    warmup_ratio=1,
    warmup_iters=5,
    num_last_epochs=15,
    min_lr_ratio=0.05)
runner = dict(type='EpochBasedRunner', max_epochs=300)
checkpoint_config = dict(interval=10)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [
    dict(type='YOLOXModeSwitchHook', num_last_epochs=15, priority=48),
    dict(type='SyncNormHook', num_last_epochs=15, interval=10, priority=48),
    dict(
        type='ExpMomentumEMAHook',
        resume_from=None,
        momentum=0.0001,
        priority=49)
]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
img_scale = (640, 640)
model = dict(
    type='YOLOX',
    input_size=(640, 640),
    random_size_range=(15, 25),
    random_size_interval=10,
    backbone=dict(type='CSPDarknet', deepen_factor=0.33, widen_factor=0.5),
    neck=dict(
        type='YOLOXPAFPN',
        in_channels=[128, 256, 512],
        out_channels=128,
        num_csp_blocks=1),
    bbox_head=dict(
        type='YOLOXHead', num_classes=1, in_channels=128, feat_channels=128),
    train_cfg=dict(assigner=dict(type='SimOTAAssigner', center_radius=2.5)),
    test_cfg=dict(score_thr=0.01, nms=dict(type='nms', iou_threshold=0.65)))
data_root = '/home/liyanni/1307/ljw/flow/'
dataset_type = 'CocoDataset'
train_pipeline = [
    dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),
    dict(
        type='RandomAffine', scaling_ratio_range=(0.1, 2),
        border=(-320, -320)),
    dict(
        type='MixUp',
        img_scale=(640, 640),
        ratio_range=(0.8, 1.6),
        pad_val=114.0),
    dict(type='YOLOXHSVRandomAug'),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(type='Resize', img_scale=(640, 640), keep_ratio=True),
    dict(
        type='Pad',
        pad_to_square=True,
        pad_val=dict(img=(114.0, 114.0, 114.0))),
    dict(type='FilterAnnotations', min_gt_bbox_wh=(1, 1), keep_empty=False),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
]
classes = 'bottle'
train_dataset = dict(
    type='MultiImageMixDataset',
    dataset=dict(
        type='CocoDataset',
        ann_file='/home/liyanni/1307/ljw/flow/annotations/train.json',
        img_prefix='/home/liyanni/1307/ljw/flow/FloW_IMG/training/images/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True)
        ],
        filter_empty_gt=False),
    pipeline=[
        dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),
        dict(
            type='RandomAffine',
            scaling_ratio_range=(0.1, 2),
            border=(-320, -320)),
        dict(
            type='MixUp',
            img_scale=(640, 640),
            ratio_range=(0.8, 1.6),
            pad_val=114.0),
        dict(type='YOLOXHSVRandomAug'),
        dict(type='RandomFlip', flip_ratio=0.5),
        dict(type='Resize', img_scale=(640, 640), keep_ratio=True),
        dict(
            type='Pad',
            pad_to_square=True,
            pad_val=dict(img=(114.0, 114.0, 114.0))),
        dict(
            type='FilterAnnotations', min_gt_bbox_wh=(1, 1), keep_empty=False),
        dict(type='DefaultFormatBundle'),
        dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
    ])
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(640, 640),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Pad',
                pad_to_square=True,
                pad_val=dict(img=(114.0, 114.0, 114.0))),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=4,
    persistent_workers=True,
    classes='bottle',
    train=dict(
        type='MultiImageMixDataset',
        dataset=dict(
            type='CocoDataset',
            ann_file='/home/liyanni/1307/ljw/flow/annotations/train.json',
            img_prefix='/home/liyanni/1307/ljw/flow/FloW_IMG/training/images/',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(type='LoadAnnotations', with_bbox=True)
            ],
            filter_empty_gt=False),
        pipeline=[
            dict(type='Mosaic', img_scale=(640, 640), pad_val=114.0),
            dict(
                type='RandomAffine',
                scaling_ratio_range=(0.1, 2),
                border=(-320, -320)),
            dict(
                type='MixUp',
                img_scale=(640, 640),
                ratio_range=(0.8, 1.6),
                pad_val=114.0),
            dict(type='YOLOXHSVRandomAug'),
            dict(type='RandomFlip', flip_ratio=0.5),
            dict(type='Resize', img_scale=(640, 640), keep_ratio=True),
            dict(
                type='Pad',
                pad_to_square=True,
                pad_val=dict(img=(114.0, 114.0, 114.0))),
            dict(
                type='FilterAnnotations',
                min_gt_bbox_wh=(1, 1),
                keep_empty=False),
            dict(type='DefaultFormatBundle'),
            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])
        ]),
    val=dict(
        type='CocoDataset',
        ann_file='/home/liyanni/1307/ljw/flow/annotations/test.json',
        img_prefix='/home/liyanni/1307/ljw/flow/FloW_IMG/test/images/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(640, 640),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Pad',
                        pad_to_square=True,
                        pad_val=dict(img=(114.0, 114.0, 114.0))),
                    dict(type='DefaultFormatBundle'),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        ann_file='/home/liyanni/1307/ljw/flow/annotations/test.json',
        img_prefix='/home/liyanni/1307/ljw/flow/FloW_IMG/test/images/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(640, 640),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Pad',
                        pad_to_square=True,
                        pad_val=dict(img=(114.0, 114.0, 114.0))),
                    dict(type='DefaultFormatBundle'),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
max_epochs = 300
num_last_epochs = 15
interval = 10
evaluation = dict(
    save_best='auto', interval=10, dynamic_intervals=[(285, 1)], metric='bbox')
work_dir = 'cls_out'
auto_resume = False
gpu_ids = [1]

2022-02-13 15:59:12,971 - mmdet - INFO - Set random seed to 1643868254, deterministic: False
2022-02-13 15:59:13,482 - mmdet - INFO - initialize CSPDarknet with init_cfg {'type': 'Kaiming', 'layer': 'Conv2d', 'a': 2.23606797749979, 'distribution': 'uniform', 'mode': 'fan_in', 'nonlinearity': 'leaky_relu'}
=> no checkpoint
Sun Feb 13 15:59:37 2022, Epoch: 1, loss: 0.7347
Sun Feb 13 15:59:50 2022, Epoch: 1, loss: 0.6868
Sun Feb 13 16:00:01 2022, Epoch: 1, loss: 0.6429
Sun Feb 13 16:00:07 2022, Epoch: 2, loss: 0.5081
Sun Feb 13 16:00:12 2022, Epoch: 2, loss: 0.5074
Sun Feb 13 16:00:16 2022, Epoch: 2, loss: 0.4866
Sun Feb 13 16:00:22 2022, Epoch: 3, loss: 0.3861
Sun Feb 13 16:00:28 2022, Epoch: 3, loss: 0.3953
Sun Feb 13 16:00:32 2022, Epoch: 3, loss: 0.4031
Sun Feb 13 16:00:37 2022, Epoch: 4, loss: 0.3656
Sun Feb 13 16:00:43 2022, Epoch: 4, loss: 0.3593
Sun Feb 13 16:00:48 2022, Epoch: 4, loss: 0.3469
Sun Feb 13 16:00:53 2022, Epoch: 5, loss: 0.3467
Sun Feb 13 16:00:58 2022, Epoch: 5, loss: 0.3093
Sun Feb 13 16:01:03 2022, Epoch: 5, loss: 0.3384
Sun Feb 13 16:01:08 2022, Epoch: 6, loss: 0.3523
Sun Feb 13 16:01:13 2022, Epoch: 6, loss: 0.3381
Sun Feb 13 16:01:18 2022, Epoch: 6, loss: 0.3408
Sun Feb 13 16:01:23 2022, Epoch: 7, loss: 0.2655
Sun Feb 13 16:01:28 2022, Epoch: 7, loss: 0.2747
Sun Feb 13 16:01:33 2022, Epoch: 7, loss: 0.2803
Sun Feb 13 16:01:38 2022, Epoch: 8, loss: 0.2510
Sun Feb 13 16:01:44 2022, Epoch: 8, loss: 0.2559
Sun Feb 13 16:01:48 2022, Epoch: 8, loss: 0.2520
Sun Feb 13 16:01:53 2022, Epoch: 9, loss: 0.2569
Sun Feb 13 16:01:59 2022, Epoch: 9, loss: 0.2251
Sun Feb 13 16:02:03 2022, Epoch: 9, loss: 0.2355
Save Model
Sun Feb 13 16:02:21 2022, Epoch: 10, loss: 0.2132
Sun Feb 13 16:02:26 2022, Epoch: 10, loss: 0.2034
Sun Feb 13 16:02:31 2022, Epoch: 10, loss: 0.2157
Sun Feb 13 16:02:35 2022, Epoch: 11, loss: 0.2088
Sun Feb 13 16:02:42 2022, Epoch: 11, loss: 0.2178
Sun Feb 13 16:02:46 2022, Epoch: 11, loss: 0.2061
Sun Feb 13 16:02:52 2022, Epoch: 12, loss: 0.1712
Sun Feb 13 16:02:57 2022, Epoch: 12, loss: 0.1534
Sun Feb 13 16:03:02 2022, Epoch: 12, loss: 0.1625
Sun Feb 13 16:03:07 2022, Epoch: 13, loss: 0.1689
Sun Feb 13 16:03:12 2022, Epoch: 13, loss: 0.1654
Sun Feb 13 16:03:16 2022, Epoch: 13, loss: 0.1818
Sun Feb 13 16:03:21 2022, Epoch: 14, loss: 0.2199
Sun Feb 13 16:03:27 2022, Epoch: 14, loss: 0.2342
Sun Feb 13 16:03:31 2022, Epoch: 14, loss: 0.2407
Sun Feb 13 16:03:36 2022, Epoch: 15, loss: 0.2259
Sun Feb 13 16:03:41 2022, Epoch: 15, loss: 0.2217
Sun Feb 13 16:03:46 2022, Epoch: 15, loss: 0.2220
Sun Feb 13 16:03:51 2022, Epoch: 16, loss: 0.2109
Sun Feb 13 16:03:56 2022, Epoch: 16, loss: 0.2283
Sun Feb 13 16:04:01 2022, Epoch: 16, loss: 0.2222
Sun Feb 13 16:04:06 2022, Epoch: 17, loss: 0.2331
Sun Feb 13 16:04:12 2022, Epoch: 17, loss: 0.2025
Sun Feb 13 16:04:17 2022, Epoch: 17, loss: 0.1999
Sun Feb 13 16:04:22 2022, Epoch: 18, loss: 0.1310
Sun Feb 13 16:04:28 2022, Epoch: 18, loss: 0.1558
Sun Feb 13 16:04:32 2022, Epoch: 18, loss: 0.1647
Sun Feb 13 16:04:38 2022, Epoch: 19, loss: 0.1617
Sun Feb 13 16:04:43 2022, Epoch: 19, loss: 0.1628
Sun Feb 13 16:04:47 2022, Epoch: 19, loss: 0.1653
Save Model
Sun Feb 13 16:04:56 2022, Epoch: 20, loss: 0.1770
Sun Feb 13 16:05:01 2022, Epoch: 20, loss: 0.1713
Sun Feb 13 16:05:05 2022, Epoch: 20, loss: 0.1706
Sun Feb 13 16:05:11 2022, Epoch: 21, loss: 0.1428
Sun Feb 13 16:05:16 2022, Epoch: 21, loss: 0.1449
Sun Feb 13 16:05:21 2022, Epoch: 21, loss: 0.1434
Sun Feb 13 16:05:26 2022, Epoch: 22, loss: 0.1232
Sun Feb 13 16:05:32 2022, Epoch: 22, loss: 0.1147
Sun Feb 13 16:05:36 2022, Epoch: 22, loss: 0.1213
Sun Feb 13 16:05:41 2022, Epoch: 23, loss: 0.1577
Sun Feb 13 16:05:47 2022, Epoch: 23, loss: 0.1627
Sun Feb 13 16:05:51 2022, Epoch: 23, loss: 0.1802
Sun Feb 13 16:05:57 2022, Epoch: 24, loss: 0.2068
Sun Feb 13 16:06:02 2022, Epoch: 24, loss: 0.1727
Sun Feb 13 16:06:07 2022, Epoch: 24, loss: 0.1983
Sun Feb 13 16:06:12 2022, Epoch: 25, loss: 0.1547
Sun Feb 13 16:06:17 2022, Epoch: 25, loss: 0.1922
Sun Feb 13 16:06:22 2022, Epoch: 25, loss: 0.1863
Sun Feb 13 16:06:27 2022, Epoch: 26, loss: 0.1671
Sun Feb 13 16:06:32 2022, Epoch: 26, loss: 0.1607
Sun Feb 13 16:06:36 2022, Epoch: 26, loss: 0.1594
Sun Feb 13 16:06:42 2022, Epoch: 27, loss: 0.1341
Sun Feb 13 16:06:47 2022, Epoch: 27, loss: 0.1337
Sun Feb 13 16:06:51 2022, Epoch: 27, loss: 0.1256
Sun Feb 13 16:06:57 2022, Epoch: 28, loss: 0.1196
Sun Feb 13 16:07:03 2022, Epoch: 28, loss: 0.1219
Sun Feb 13 16:07:07 2022, Epoch: 28, loss: 0.1185
Sun Feb 13 16:07:13 2022, Epoch: 29, loss: 0.0873
Sun Feb 13 16:07:18 2022, Epoch: 29, loss: 0.0989
Sun Feb 13 16:07:23 2022, Epoch: 29, loss: 0.1103
Save Model
Sun Feb 13 16:07:31 2022, Epoch: 30, loss: 0.0648
Sun Feb 13 16:07:36 2022, Epoch: 30, loss: 0.0933
Sun Feb 13 16:07:40 2022, Epoch: 30, loss: 0.0900
Sun Feb 13 16:07:46 2022, Epoch: 31, loss: 0.0786
Sun Feb 13 16:07:51 2022, Epoch: 31, loss: 0.0804
Sun Feb 13 16:07:56 2022, Epoch: 31, loss: 0.0994
Sun Feb 13 16:08:02 2022, Epoch: 32, loss: 0.1533
Sun Feb 13 16:08:07 2022, Epoch: 32, loss: 0.1431
Sun Feb 13 16:08:12 2022, Epoch: 32, loss: 0.1333
Sun Feb 13 16:08:17 2022, Epoch: 33, loss: 0.0805
Sun Feb 13 16:08:23 2022, Epoch: 33, loss: 0.1020
Sun Feb 13 16:08:27 2022, Epoch: 33, loss: 0.1155
Sun Feb 13 16:08:33 2022, Epoch: 34, loss: 0.1032
Sun Feb 13 16:08:38 2022, Epoch: 34, loss: 0.0952
Sun Feb 13 16:08:43 2022, Epoch: 34, loss: 0.1047
Sun Feb 13 16:08:48 2022, Epoch: 35, loss: 0.1174
Sun Feb 13 16:08:53 2022, Epoch: 35, loss: 0.1540
Sun Feb 13 16:08:58 2022, Epoch: 35, loss: 0.1338
Sun Feb 13 16:09:04 2022, Epoch: 36, loss: 0.1154
Sun Feb 13 16:09:09 2022, Epoch: 36, loss: 0.1006
Sun Feb 13 16:09:13 2022, Epoch: 36, loss: 0.1057
Sun Feb 13 16:09:19 2022, Epoch: 37, loss: 0.1280
Sun Feb 13 16:09:24 2022, Epoch: 37, loss: 0.1098
Sun Feb 13 16:09:29 2022, Epoch: 37, loss: 0.1308
Sun Feb 13 16:09:34 2022, Epoch: 38, loss: 0.1258
Sun Feb 13 16:09:40 2022, Epoch: 38, loss: 0.1597
Sun Feb 13 16:09:44 2022, Epoch: 38, loss: 0.1561
Sun Feb 13 16:09:49 2022, Epoch: 39, loss: 0.0773
Sun Feb 13 16:09:54 2022, Epoch: 39, loss: 0.0897
Sun Feb 13 16:09:59 2022, Epoch: 39, loss: 0.0926
Save Model
Sun Feb 13 16:10:07 2022, Epoch: 40, loss: 0.0765
Sun Feb 13 16:10:12 2022, Epoch: 40, loss: 0.0808
Sun Feb 13 16:10:16 2022, Epoch: 40, loss: 0.0895
Sun Feb 13 16:10:21 2022, Epoch: 41, loss: 0.0722
Sun Feb 13 16:10:27 2022, Epoch: 41, loss: 0.0749
Sun Feb 13 16:10:31 2022, Epoch: 41, loss: 0.0774
Sun Feb 13 16:10:36 2022, Epoch: 42, loss: 0.0673
Sun Feb 13 16:10:42 2022, Epoch: 42, loss: 0.0681
Sun Feb 13 16:10:46 2022, Epoch: 42, loss: 0.0696
Sun Feb 13 16:10:51 2022, Epoch: 43, loss: 0.0579
Sun Feb 13 16:10:57 2022, Epoch: 43, loss: 0.0739
Sun Feb 13 16:11:01 2022, Epoch: 43, loss: 0.0676
Sun Feb 13 16:11:07 2022, Epoch: 44, loss: 0.0413
Sun Feb 13 16:11:12 2022, Epoch: 44, loss: 0.0448
Sun Feb 13 16:11:16 2022, Epoch: 44, loss: 0.0698
Sun Feb 13 16:11:22 2022, Epoch: 45, loss: 0.1793
Sun Feb 13 16:11:27 2022, Epoch: 45, loss: 0.1613
Sun Feb 13 16:11:31 2022, Epoch: 45, loss: 0.1456
Sun Feb 13 16:11:36 2022, Epoch: 46, loss: 0.0923
Sun Feb 13 16:11:42 2022, Epoch: 46, loss: 0.0859
Sun Feb 13 16:11:47 2022, Epoch: 46, loss: 0.0858
Sun Feb 13 16:11:53 2022, Epoch: 47, loss: 0.0654
Sun Feb 13 16:11:59 2022, Epoch: 47, loss: 0.0643
Sun Feb 13 16:12:04 2022, Epoch: 47, loss: 0.0688
Sun Feb 13 16:12:09 2022, Epoch: 48, loss: 0.0607
Sun Feb 13 16:12:15 2022, Epoch: 48, loss: 0.0673
Sun Feb 13 16:12:19 2022, Epoch: 48, loss: 0.0601
Sun Feb 13 16:12:24 2022, Epoch: 49, loss: 0.0686
Sun Feb 13 16:12:29 2022, Epoch: 49, loss: 0.0674
Sun Feb 13 16:12:34 2022, Epoch: 49, loss: 0.0562
Save Model
Sun Feb 13 16:12:44 2022, Epoch: 50, loss: 0.0456
Sun Feb 13 16:12:50 2022, Epoch: 50, loss: 0.0499
Sun Feb 13 16:12:55 2022, Epoch: 50, loss: 0.0693
Sun Feb 13 16:13:01 2022, Epoch: 51, loss: 0.1191
Sun Feb 13 16:13:06 2022, Epoch: 51, loss: 0.1142
Sun Feb 13 16:13:10 2022, Epoch: 51, loss: 0.1230
Sun Feb 13 16:13:15 2022, Epoch: 52, loss: 0.2297
Sun Feb 13 16:13:20 2022, Epoch: 52, loss: 0.2086
Sun Feb 13 16:13:25 2022, Epoch: 52, loss: 0.2308
Sun Feb 13 16:13:30 2022, Epoch: 53, loss: 0.1634
Sun Feb 13 16:13:35 2022, Epoch: 53, loss: 0.1533
Sun Feb 13 16:13:39 2022, Epoch: 53, loss: 0.1567
Sun Feb 13 16:13:44 2022, Epoch: 54, loss: 0.0912
Sun Feb 13 16:13:49 2022, Epoch: 54, loss: 0.0931
Sun Feb 13 16:13:54 2022, Epoch: 54, loss: 0.0939
Sun Feb 13 16:13:59 2022, Epoch: 55, loss: 0.0624
Sun Feb 13 16:14:04 2022, Epoch: 55, loss: 0.0659
Sun Feb 13 16:14:09 2022, Epoch: 55, loss: 0.0600
Sun Feb 13 16:14:16 2022, Epoch: 56, loss: 0.0287
Sun Feb 13 16:14:22 2022, Epoch: 56, loss: 0.0351
Sun Feb 13 16:14:28 2022, Epoch: 56, loss: 0.0811
Sun Feb 13 16:14:33 2022, Epoch: 57, loss: 0.1750
Sun Feb 13 16:14:40 2022, Epoch: 57, loss: 0.2171
Sun Feb 13 16:14:45 2022, Epoch: 57, loss: 0.2056
Sun Feb 13 16:14:51 2022, Epoch: 58, loss: 0.1734
Sun Feb 13 16:14:56 2022, Epoch: 58, loss: 0.1639
Sun Feb 13 16:15:01 2022, Epoch: 58, loss: 0.1410
Sun Feb 13 16:15:06 2022, Epoch: 59, loss: 0.1126
Sun Feb 13 16:15:13 2022, Epoch: 59, loss: 0.1078
Sun Feb 13 16:15:17 2022, Epoch: 59, loss: 0.1009
Sun Feb 13 16:15:27 2022, Epoch: 60, loss: 0.0623
Sun Feb 13 16:15:32 2022, Epoch: 60, loss: 0.0795
Sun Feb 13 16:15:36 2022, Epoch: 60, loss: 0.0742
Sun Feb 13 16:15:42 2022, Epoch: 61, loss: 0.0774
Sun Feb 13 16:15:47 2022, Epoch: 61, loss: 0.0652
Sun Feb 13 16:15:53 2022, Epoch: 61, loss: 0.0691
Sun Feb 13 16:16:00 2022, Epoch: 62, loss: 0.0463
Sun Feb 13 16:16:06 2022, Epoch: 62, loss: 0.0514
Sun Feb 13 16:16:10 2022, Epoch: 62, loss: 0.0710
Sun Feb 13 16:16:16 2022, Epoch: 63, loss: 0.0715
Sun Feb 13 16:16:21 2022, Epoch: 63, loss: 0.0824
Sun Feb 13 16:16:25 2022, Epoch: 63, loss: 0.0953
Sun Feb 13 16:16:31 2022, Epoch: 64, loss: 0.0803
Sun Feb 13 16:16:36 2022, Epoch: 64, loss: 0.0773
Sun Feb 13 16:16:40 2022, Epoch: 64, loss: 0.0646
Sun Feb 13 16:16:46 2022, Epoch: 65, loss: 0.0571
Sun Feb 13 16:16:52 2022, Epoch: 65, loss: 0.0633
Sun Feb 13 16:16:56 2022, Epoch: 65, loss: 0.0551
Sun Feb 13 16:17:02 2022, Epoch: 66, loss: 0.0404
Sun Feb 13 16:17:08 2022, Epoch: 66, loss: 0.0433
Sun Feb 13 16:17:14 2022, Epoch: 66, loss: 0.0422
Sun Feb 13 16:17:20 2022, Epoch: 67, loss: 0.0358
Sun Feb 13 16:17:26 2022, Epoch: 67, loss: 0.0375
Sun Feb 13 16:17:31 2022, Epoch: 67, loss: 0.0419
Sun Feb 13 16:17:36 2022, Epoch: 68, loss: 0.0447
Sun Feb 13 16:17:42 2022, Epoch: 68, loss: 0.0443
Sun Feb 13 16:17:46 2022, Epoch: 68, loss: 0.0653
Sun Feb 13 16:17:52 2022, Epoch: 69, loss: 0.0764
Sun Feb 13 16:17:57 2022, Epoch: 69, loss: 0.0828
Sun Feb 13 16:18:02 2022, Epoch: 69, loss: 0.0868
Sun Feb 13 16:18:11 2022, Epoch: 70, loss: 0.0784
Sun Feb 13 16:18:16 2022, Epoch: 70, loss: 0.0650
Sun Feb 13 16:18:21 2022, Epoch: 70, loss: 0.0806
Sun Feb 13 16:18:26 2022, Epoch: 71, loss: 0.0703
Sun Feb 13 16:18:31 2022, Epoch: 71, loss: 0.1193
Sun Feb 13 16:18:36 2022, Epoch: 71, loss: 0.1298
Sun Feb 13 16:18:42 2022, Epoch: 72, loss: 0.0900
Sun Feb 13 16:18:47 2022, Epoch: 72, loss: 0.1098
Sun Feb 13 16:18:52 2022, Epoch: 72, loss: 0.1084
Sun Feb 13 16:18:57 2022, Epoch: 73, loss: 0.0500
Sun Feb 13 16:19:03 2022, Epoch: 73, loss: 0.0557
Sun Feb 13 16:19:08 2022, Epoch: 73, loss: 0.0528
Sun Feb 13 16:19:13 2022, Epoch: 74, loss: 0.0227
Sun Feb 13 16:19:18 2022, Epoch: 74, loss: 0.0401
Sun Feb 13 16:19:23 2022, Epoch: 74, loss: 0.0387
Sun Feb 13 16:19:28 2022, Epoch: 75, loss: 0.0253
Sun Feb 13 16:19:34 2022, Epoch: 75, loss: 0.0263
Sun Feb 13 16:19:38 2022, Epoch: 75, loss: 0.0268
Sun Feb 13 16:19:44 2022, Epoch: 76, loss: 0.0243
Sun Feb 13 16:19:49 2022, Epoch: 76, loss: 0.0239
Sun Feb 13 16:19:54 2022, Epoch: 76, loss: 0.0219
Sun Feb 13 16:19:59 2022, Epoch: 77, loss: 0.0187
Sun Feb 13 16:20:04 2022, Epoch: 77, loss: 0.0237
Sun Feb 13 16:20:08 2022, Epoch: 77, loss: 0.0458
Sun Feb 13 16:20:13 2022, Epoch: 78, loss: 0.0852
Sun Feb 13 16:20:18 2022, Epoch: 78, loss: 0.0919
Sun Feb 13 16:20:23 2022, Epoch: 78, loss: 0.1369
Sun Feb 13 16:20:28 2022, Epoch: 79, loss: 0.1145
Sun Feb 13 16:20:33 2022, Epoch: 79, loss: 0.1309
Sun Feb 13 16:20:38 2022, Epoch: 79, loss: 0.1267
Save Model
Sun Feb 13 16:20:48 2022, Epoch: 80, loss: 0.0979
Sun Feb 13 16:20:53 2022, Epoch: 80, loss: 0.0831
Sun Feb 13 16:20:57 2022, Epoch: 80, loss: 0.0766
Sun Feb 13 16:21:03 2022, Epoch: 81, loss: 0.0485
Sun Feb 13 16:21:08 2022, Epoch: 81, loss: 0.0491
Sun Feb 13 16:21:12 2022, Epoch: 81, loss: 0.0448
Sun Feb 13 16:21:18 2022, Epoch: 82, loss: 0.0214
Sun Feb 13 16:21:23 2022, Epoch: 82, loss: 0.0292
Sun Feb 13 16:21:28 2022, Epoch: 82, loss: 0.0262
Sun Feb 13 16:21:33 2022, Epoch: 83, loss: 0.0243
Sun Feb 13 16:21:39 2022, Epoch: 83, loss: 0.0292
Sun Feb 13 16:21:43 2022, Epoch: 83, loss: 0.0571
Sun Feb 13 16:21:49 2022, Epoch: 84, loss: 0.1290
Sun Feb 13 16:21:55 2022, Epoch: 84, loss: 0.1682
Sun Feb 13 16:21:59 2022, Epoch: 84, loss: 0.1535
Sun Feb 13 16:22:05 2022, Epoch: 85, loss: 0.1135
Sun Feb 13 16:22:10 2022, Epoch: 85, loss: 0.1154
Sun Feb 13 16:22:15 2022, Epoch: 85, loss: 0.1062
Sun Feb 13 16:22:20 2022, Epoch: 86, loss: 0.0670
Sun Feb 13 16:22:26 2022, Epoch: 86, loss: 0.0764
Sun Feb 13 16:22:30 2022, Epoch: 86, loss: 0.0774
Sun Feb 13 16:22:36 2022, Epoch: 87, loss: 0.0418
Sun Feb 13 16:22:41 2022, Epoch: 87, loss: 0.0573
Sun Feb 13 16:22:45 2022, Epoch: 87, loss: 0.0528
Sun Feb 13 16:22:50 2022, Epoch: 88, loss: 0.0409
Sun Feb 13 16:22:56 2022, Epoch: 88, loss: 0.0311
Sun Feb 13 16:23:00 2022, Epoch: 88, loss: 0.0298
Sun Feb 13 16:23:05 2022, Epoch: 89, loss: 0.0406
Sun Feb 13 16:23:10 2022, Epoch: 89, loss: 0.0338
Sun Feb 13 16:23:15 2022, Epoch: 89, loss: 0.0389
Sun Feb 13 16:23:23 2022, Epoch: 90, loss: 0.0502
Sun Feb 13 16:23:29 2022, Epoch: 90, loss: 0.0518
Sun Feb 13 16:23:34 2022, Epoch: 90, loss: 0.0541
Sun Feb 13 16:23:40 2022, Epoch: 91, loss: 0.0436
Sun Feb 13 16:23:46 2022, Epoch: 91, loss: 0.0424
Sun Feb 13 16:23:51 2022, Epoch: 91, loss: 0.0688
Sun Feb 13 16:23:57 2022, Epoch: 92, loss: 0.0846
Sun Feb 13 16:24:02 2022, Epoch: 92, loss: 0.0643
Sun Feb 13 16:24:06 2022, Epoch: 92, loss: 0.0612
Sun Feb 13 16:24:12 2022, Epoch: 93, loss: 0.0402
Sun Feb 13 16:24:19 2022, Epoch: 93, loss: 0.0522
Sun Feb 13 16:24:23 2022, Epoch: 93, loss: 0.0501
Sun Feb 13 16:24:29 2022, Epoch: 94, loss: 0.0303
Sun Feb 13 16:24:35 2022, Epoch: 94, loss: 0.0381
Sun Feb 13 16:24:39 2022, Epoch: 94, loss: 0.0450
Sun Feb 13 16:24:44 2022, Epoch: 95, loss: 0.0572
Sun Feb 13 16:24:50 2022, Epoch: 95, loss: 0.0660
Sun Feb 13 16:24:54 2022, Epoch: 95, loss: 0.0614
Sun Feb 13 16:24:59 2022, Epoch: 96, loss: 0.0614
Sun Feb 13 16:25:05 2022, Epoch: 96, loss: 0.0554
Sun Feb 13 16:25:09 2022, Epoch: 96, loss: 0.0501
Sun Feb 13 16:25:15 2022, Epoch: 97, loss: 0.0183
Sun Feb 13 16:25:20 2022, Epoch: 97, loss: 0.0251
Sun Feb 13 16:25:25 2022, Epoch: 97, loss: 0.0241
Sun Feb 13 16:25:30 2022, Epoch: 98, loss: 0.0125
Sun Feb 13 16:25:36 2022, Epoch: 98, loss: 0.0176
Sun Feb 13 16:25:40 2022, Epoch: 98, loss: 0.0171
Sun Feb 13 16:25:45 2022, Epoch: 99, loss: 0.0071
Sun Feb 13 16:25:50 2022, Epoch: 99, loss: 0.0149
Sun Feb 13 16:25:55 2022, Epoch: 99, loss: 0.0182
Sun Feb 13 16:26:07 2022, Epoch: 100, loss: 0.0143
Sun Feb 13 16:26:14 2022, Epoch: 100, loss: 0.0458
Sun Feb 13 16:26:19 2022, Epoch: 100, loss: 0.0539
